{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author's description:\n",
    "\n",
    "auto-sklearn is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator\n",
    "\n",
    "#### Useful links:\n",
    "\n",
    "[install link](https://automl.github.io/auto-sklearn/master/installation.html),\n",
    "[git](https://github.com/automl/auto-sklearn),\n",
    "[manual](https://automl.github.io/auto-sklearn/master/manual.html),\n",
    "[parallel instances](https://automl.github.io/auto-sklearn/master/examples/example_parallel_manual_spawning.html),\n",
    "[parallel runs on one machine](https://automl.github.io/auto-sklearn/master/examples/example_parallel_n_jobs.html),\n",
    "[cross validation](https://automl.github.io/auto-sklearn/master/examples/example_crossvalidation.html),\n",
    "[feature types](https://automl.github.io/auto-sklearn/master/examples/example_feature_types.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto-sklearn builds ensembles, provides good control of the auto-ML run details, and makes exporting easy due to its scikit-learn foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "  Downloading auto-sklearn-0.14.7.tar.gz (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (60.9.3)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (4.1.1)\n",
      "Collecting distro\n",
      "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (1.7.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (1.1.0)\n",
      "Collecting scikit-learn<0.25.0,>=0.24.0\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.9 MB 93.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dask>=2021.12\n",
      "  Downloading dask-2022.8.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 91.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting distributed>=2012.12\n",
      "  Downloading distributed-2022.8.0-py3-none-any.whl (890 kB)\n",
      "\u001b[K     |████████████████████████████████| 890 kB 90.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[K     |████████████████████████████████| 701 kB 86.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0 in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (1.3.5)\n",
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "Requirement already satisfied: threadpoolctl in /opt/conda/lib/python3.8/site-packages (from auto-sklearn) (3.1.0)\n",
      "Collecting ConfigSpace<0.5,>=0.4.21\n",
      "  Downloading ConfigSpace-0.4.21-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 83.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pynisher<0.7,>=0.6.3\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
      "Collecting pyrfr<0.9,>=0.8.1\n",
      "  Downloading pyrfr-0.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 98.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smac<1.3,>=1.2\n",
      "  Downloading smac-1.2.tar.gz (260 kB)\n",
      "\u001b[K     |████████████████████████████████| 260 kB 106.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing in /opt/conda/lib/python3.8/site-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (3.0.7)\n",
      "Collecting cython\n",
      "  Downloading Cython-0.29.32-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 94.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=0.6.0\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 109.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting partd>=0.3.10\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 11.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask>=2021.12->auto-sklearn) (21.3)\n",
      "Requirement already satisfied: click>=6.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from distributed>=2012.12->auto-sklearn) (8.0.4)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed>=2012.12->auto-sklearn) (1.26.6)\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.9.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 98.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tornado<6.2,>=6.0.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from distributed>=2012.12->auto-sklearn) (6.1)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting locket>=1.0.0\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[K     |████████████████████████████████| 322 kB 105.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.8/site-packages (from distributed>=2012.12->auto-sklearn) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0->auto-sklearn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.16.0)\n",
      "Collecting emcee>=3.0.0\n",
      "  Downloading emcee-3.1.2-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from jinja2->distributed>=2012.12->auto-sklearn) (2.1.0)\n",
      "Building wheels for collected packages: auto-sklearn, pynisher, smac, liac-arff\n",
      "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-sklearn: filename=auto_sklearn-0.14.7-py3-none-any.whl size=6602873 sha256=ea88ac28fbf36a4c7144649b39a83000a95f7f1e43cb587a8652dc5d4fbab881\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/ad/44/ae/830c6d6190da34d4c9d8bdc1a26e2c84bd30b606181530955b\n",
      "  Building wheel for pynisher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7043 sha256=c734f0df714df48cfe40570589bf94808356e8d6c886bccef460bf89f60b376c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a7/0b/c3/169e35bcd72f20d0d5e24c37dd2dff8282cc16c06df9762ff5\n",
      "  Building wheel for smac (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smac: filename=smac-1.2-py3-none-any.whl size=215916 sha256=4f1bf0d0f3f737399421472362e5d11ce94ea2ea8318cfad2ee56962461c2b9b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1c/0d/63/29515e546f52561bf5ff41e3968fe2c35afe4ae366be54b2c4\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11733 sha256=53f6778c36ff227495c3d0513f54f5362ee50a9b8bc0045c5406eaa57984665b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a2/de/68/bf3972de3ecb31e32bef59a7f4c75f0687a3674c476b347c14\n",
      "Successfully built auto-sklearn pynisher smac liac-arff\n",
      "Installing collected packages: toolz, locket, pyyaml, partd, heapdict, fsspec, cloudpickle, zict, tblib, sortedcontainers, psutil, msgpack, dask, cython, scikit-learn, pyrfr, pynisher, emcee, distributed, ConfigSpace, smac, liac-arff, distro, auto-sklearn\n",
      "Successfully installed ConfigSpace-0.4.21 auto-sklearn-0.14.7 cloudpickle-2.1.0 cython-0.29.32 dask-2022.8.0 distributed-2022.8.0 distro-1.7.0 emcee-3.1.2 fsspec-2022.7.1 heapdict-1.0.1 liac-arff-2.5.0 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 psutil-5.9.1 pynisher-0.6.4 pyrfr-0.8.3 pyyaml-6.0 scikit-learn-0.24.2 smac-1.2 sortedcontainers-2.4.0 tblib-1.7.0 toolz-0.12.0 zict-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install auto-sklearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noriginal verision of auto-sklearn used\\ndid not capture original versions of other libraries like sklearn\\ndoesn't work on a new compute environment due to version mismatches\\nbest approach is to try on current latest versions of everything\\nand then freeze all the package versions once it is working again\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install auto-sklearn==0.6.0 --user\n",
    "\"\"\"\n",
    "original verision of auto-sklearn used\n",
    "did not capture original versions of other libraries like sklearn\n",
    "doesn't work on a new compute environment due to version mismatches\n",
    "best approach is to try on current latest versions of everything\n",
    "and then freeze all the package versions once it is working again\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *---You may need to restart the kernel after install and run again starting here---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there can be a lot of warnings in auto-sklearn\n",
    "#especially if you overwrite existing files\n",
    "#turning off for demo purposes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the classification function\n",
    "\n",
    "auto-sklearn is mostly a wrapper around scikit-learn. It was not the intention of the authors to allow user control over details such as the modeling algorithm and typical hyper-parameter choices. Control is several layers deep in the [SMAC](https://automl.github.io/SMAC3/master/) space and scenario settings. The user can control the time is takes to build the ensemble, the resampling strategy and the parallelization of the work across CPUs on the machine. These will be demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "?autosklearn.classification.AutoSklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the heart disease dataset\n",
    "\n",
    "Note that in this cell we are calling **sklearn.model_selection.train_test_split()** twice and creating two sets of heart disease (hd) data for model fitting and testing. One is for the hd data without one hot encoding (ohe) and the other has the ohe columns. \n",
    "\n",
    "auto-sklearn accepts a list of categorical features and has several methods for treating categorical data. In this notebook we try both approaches - building ohe columns ourselves and letting auto-sklearn do its thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "/mnt/data/raw/heart.csv\n",
    "\n",
    "attribute documentation:\n",
    "      age: age in years\n",
    "      sex: sex (1 = male; 0 = female)\n",
    "      cp: chest pain type\n",
    "        -- Value 1: typical angina\n",
    "        -- Value 2: atypical angina\n",
    "        -- Value 3: non-anginal pain\n",
    "        -- Value 4: asymptomatic\n",
    "     trestbps: resting blood pressure (in mm Hg on admission to the \n",
    "        hospital)\n",
    "     chol: serum cholestoral in mg/dl\n",
    "     fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n",
    "     restecg: resting electrocardiographic results\n",
    "        -- Value 0: normal\n",
    "        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST \n",
    "                    elevation or depression of > 0.05 mV)\n",
    "        -- Value 2: showing probable or definite left ventricular hypertrophy\n",
    "                    by Estes' criteria\n",
    "     thalach: maximum heart rate achieved\n",
    "     exang: exercise induced angina (1 = yes; 0 = no)\n",
    "     oldpeak = ST depression induced by exercise relative to rest\n",
    "     slope: the slope of the peak exercise ST segment\n",
    "        -- Value 1: upsloping\n",
    "        -- Value 2: flat\n",
    "        -- Value 3: downsloping\n",
    "     ca: number of major vessels (0-3) colored by flourosopy\n",
    "     thal: \n",
    "         3 = normal; \n",
    "         6 = fixed defect; \n",
    "         7 = reversable defect\n",
    "     target: diagnosis of heart disease (angiographic disease status)\n",
    "        -- Value 0: < 50% diameter narrowing\n",
    "        -- Value 1: > 50% diameter narrowing\n",
    " '''\n",
    "\n",
    "#load and clean the data----------------------\n",
    "\n",
    "#column names\n",
    "names = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang', \\\n",
    "         'oldpeak','slope','ca','thal','target']\n",
    "\n",
    "#load data from Domino project directory\n",
    "hd_data = pd.read_csv(\"./data/raw/heart.csv\", header=None, names=names)\n",
    "\n",
    "#in case some data comes in as string\n",
    "#convert to numeric and coerce errors to NaN\n",
    "for col in hd_data.columns:  # Iterate over chosen columns\n",
    "    hd_data[col] = pd.to_numeric(hd_data[col], errors='coerce')\n",
    "    \n",
    "#drop nulls\n",
    "hd_data.dropna(inplace=True)\n",
    "\n",
    "#non-ohe data---------------------------------\n",
    "   \n",
    "#load the X and y set as a numpy array\n",
    "X_hd = hd_data.drop('target', axis=1).values\n",
    "y_hd = hd_data['target'].values\n",
    "\n",
    "#build the train and test sets\n",
    "X_hd_train, X_hd_test, y_hd_train, y_hd_test = \\\n",
    "    sklearn.model_selection.train_test_split(X_hd, y_hd, random_state=12)\n",
    "\n",
    "#now do ohe-----------------------------------\n",
    "\n",
    "#function to do one hot encoding for categorical columns\n",
    "def create_dummies(data, cols, drop1st=True):\n",
    "    for c in cols:\n",
    "        dummies_df = pd.get_dummies(data[c], prefix=c, drop_first=drop1st)  \n",
    "        data=pd.concat([data, dummies_df], axis=1)\n",
    "        data = data.drop([c], axis=1)\n",
    "    return data\n",
    "\n",
    "cat_cols = ['cp', 'restecg', 'slope', 'ca', 'thal']\n",
    "hd_data = create_dummies(hd_data, cat_cols)\n",
    "    \n",
    "#load the X and y set as a numpy array\n",
    "X_hd_ohe = hd_data.drop('target', axis=1).values\n",
    "y_hd_ohe = hd_data['target'].values\n",
    "\n",
    "#build the train and test sets\n",
    "X_hd_ohe_train, X_hd_ohe_test, y_hd_ohe_train, y_hd_ohe_test = \\\n",
    "    sklearn.model_selection.train_test_split(X_hd_ohe, y_hd_ohe, \\\n",
    "                                             random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model on ohe data with holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-08-11 23:40:38,894:Client-AutoML(1):heart_disease] Capping the per_run_time_limit to 29.0 to have time for a least 2 models in each process.\n",
      "CPU times: user 10.6 s, sys: 863 ms, total: 11.4 s\n",
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#build the auto-sklearn routine\n",
    "automl_hd_ohe = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=60,\n",
    "    per_run_time_limit=30,\n",
    "    disable_evaluator_output=False,\n",
    "    resampling_strategy='holdout',\n",
    "    resampling_strategy_arguments={'train_size': 0.67}\n",
    ")\n",
    "\n",
    "#call it\n",
    "automl_hd_ohe.fit(X_hd_ohe_train, y_hd_ohe_train, \\\n",
    "                  dataset_name='heart_disease')\n",
    "\n",
    "#save the predicitons\n",
    "predictions_hd_ohe = automl_hd_ohe.predict(X_hd_ohe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting with autosklearn\n",
    "\n",
    "A common mistake is to call **fit_ensemble()** after already running **fit()**. **fit()** both optimizes the machine learning models and builds an ensemble out of them. To disable ensembling when running **fit()** (with parallel instances for example) set ensemble_size to 0. Then **fit_ensemble()** would be needed once all models have been built.\n",
    "\n",
    "To save fitted models, use typical [pickle procedures](https://scikit-learn.org/stable/modules/model_persistence.html#persistence-example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "Accuracy, sprint stats, and model details are available. \n",
    "\n",
    "Later we will run auto-sklearn in parallel. Note the number of models built here and compare it to the number built with parallelization turned on. \n",
    "\n",
    "The model details give you insight into what auto-sklearn is doing under the hood. You can see the modeling algorithm used and all the parameter settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.6973684210526315\n",
      " \n",
      "-----------------------------------------\n",
      " \n",
      "Sprint Stats:\n",
      "auto-sklearn results:\n",
      "  Dataset name: heart_disease\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.880000\n",
      "  Number of target algorithm runs: 25\n",
      "  Number of successful target algorithm runs: 25\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      " \n",
      "-----------------------------------------\n",
      " \n",
      "Model Details:\n",
      "{24: {'model_id': 24, 'rank': 1, 'cost': 0.12, 'ensemble_weight': 0.14, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e5dcd58b0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e5dd2a3a0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e5dd2a2b0>, 'sklearn_classifier': AdaBoostClassifier(algorithm='SAMME',\n",
      "                   base_estimator=DecisionTreeClassifier(max_depth=2),\n",
      "                   learning_rate=0.13167493237005792, n_estimators=56,\n",
      "                   random_state=1)}, 10: {'model_id': 10, 'rank': 2, 'cost': 0.1333333333333333, 'ensemble_weight': 0.06, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e5dd2a9a0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e5dcf85e0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e5dcf8340>, 'sklearn_classifier': MLPClassifier(activation='tanh', alpha=0.0001363185819149026, beta_1=0.999,\n",
      "              beta_2=0.9, early_stopping=True,\n",
      "              hidden_layer_sizes=(115, 115, 115),\n",
      "              learning_rate_init=0.00018009776276177523, max_iter=32,\n",
      "              n_iter_no_change=32, random_state=1, verbose=0, warm_start=True)}, 13: {'model_id': 13, 'rank': 3, 'cost': 0.1333333333333333, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e5de6db50>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e5dc15a30>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e5c3a1160>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=0.4635442279519353,\n",
      "                               learning_rate=0.09809681787962342, max_iter=512,\n",
      "                               max_leaf_nodes=328, min_samples_leaf=2,\n",
      "                               n_iter_no_change=2, random_state=1,\n",
      "                               validation_fraction=None, warm_start=True)}, 3: {'model_id': 3, 'rank': 4, 'cost': 0.1466666666666666, 'ensemble_weight': 0.1, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e5dd31eb0>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e53684b80>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e5363d4c0>, 'sklearn_classifier': ExtraTreesClassifier(max_features=6, min_samples_split=10, n_estimators=512,\n",
      "                     n_jobs=1, random_state=1, warm_start=True)}, 4: {'model_id': 4, 'rank': 5, 'cost': 0.1466666666666666, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e5dc30d60>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e5363d730>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4253f3d0>, 'sklearn_classifier': MLPClassifier(activation='tanh', alpha=2.5550223982458062e-06, beta_1=0.999,\n",
      "              beta_2=0.9, hidden_layer_sizes=(54, 54, 54),\n",
      "              learning_rate_init=0.00027271287919467994, max_iter=256,\n",
      "              n_iter_no_change=32, random_state=1, validation_fraction=0.0,\n",
      "              verbose=0, warm_start=True)}, 9: {'model_id': 9, 'rank': 6, 'cost': 0.1466666666666666, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e5db22d30>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4253f1f0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e42519d90>, 'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=2, min_samples_leaf=2,\n",
      "                       n_estimators=512, n_jobs=1, random_state=1,\n",
      "                       warm_start=True)}, 26: {'model_id': 26, 'rank': 7, 'cost': 0.1466666666666666, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e536940a0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e424a2820>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e424ab3a0>, 'sklearn_classifier': GaussianNB()}, 7: {'model_id': 7, 'rank': 8, 'cost': 0.16000000000000003, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4266eaf0>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e423c4820>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4212a760>, 'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=1, n_estimators=512,\n",
      "                       n_jobs=1, random_state=1, warm_start=True)}, 2: {'model_id': 2, 'rank': 9, 'cost': 0.17333333333333334, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e424e3d30>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4212ab50>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4212afd0>, 'sklearn_classifier': RandomForestClassifier(max_features=4, n_estimators=512, n_jobs=1,\n",
      "                       random_state=1, warm_start=True)}, 16: {'model_id': 16, 'rank': 10, 'cost': 0.17333333333333334, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e424335e0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e42002af0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3fdb2430>, 'sklearn_classifier': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
      "                   learning_rate=0.046269426995092074, n_estimators=406,\n",
      "                   random_state=1)}, 5: {'model_id': 5, 'rank': 11, 'cost': 0.18666666666666665, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e42294640>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3fdb2940>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3fbd2130>, 'sklearn_classifier': RandomForestClassifier(max_features=2, min_samples_leaf=2, n_estimators=512,\n",
      "                       n_jobs=1, random_state=1, warm_start=True)}, 14: {'model_id': 14, 'rank': 12, 'cost': 0.18666666666666665, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4211b880>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3fbd2070>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3fbd26a0>, 'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=245, min_samples_leaf=2,\n",
      "                     min_samples_split=20, n_estimators=512, n_jobs=1,\n",
      "                     random_state=1, warm_start=True)}, 15: {'model_id': 15, 'rank': 13, 'cost': 0.18666666666666665, 'ensemble_weight': 0.06, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e41e45df0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3fa362b0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3f9cf1c0>, 'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=15, n_estimators=512,\n",
      "                       n_jobs=1, random_state=1, warm_start=True)}, 12: {'model_id': 12, 'rank': 14, 'cost': 0.19999999999999996, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e3fcf4af0>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3f815820>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3f815be0>, 'sklearn_classifier': MLPClassifier(alpha=0.02847755502162456, beta_1=0.999, beta_2=0.9,\n",
      "              hidden_layer_sizes=(123, 123),\n",
      "              learning_rate_init=0.000421568792103947, max_iter=256,\n",
      "              n_iter_no_change=32, random_state=1, validation_fraction=0.0,\n",
      "              verbose=0, warm_start=True)}, 18: {'model_id': 18, 'rank': 15, 'cost': 0.24, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e3fb20460>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3f761b20>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3f761d30>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=False,\n",
      "                               l2_regularization=4.821686883442146e-05,\n",
      "                               learning_rate=0.10161621495242192, max_iter=512,\n",
      "                               max_leaf_nodes=535, min_samples_leaf=10,\n",
      "                               n_iter_no_change=0, random_state=1,\n",
      "                               validation_fraction=None, warm_start=True)}, 22: {'model_id': 22, 'rank': 16, 'cost': 0.28, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e3f9343a0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3d1d6580>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3d1d6640>, 'sklearn_classifier': SVC(C=1198.7850746967626, cache_size=1946.34375, gamma=0.015219182148092949,\n",
      "    max_iter=-1.0, random_state=1, tol=0.040610448809956276)}, 19: {'model_id': 19, 'rank': 17, 'cost': 0.31999999999999995, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e3f801070>, 'balancing': Balancing(random_state=1, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3d204cd0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3d1951f0>, 'sklearn_classifier': KNeighborsClassifier(n_neighbors=1, p=1)}, 20: {'model_id': 20, 'rank': 18, 'cost': 0.43999999999999995, 'ensemble_weight': 0.06, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e3d1ce4c0>, 'balancing': Balancing(random_state=1), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e3d117a60>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e3d117d30>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=0.0015246021799433245,\n",
      "                               learning_rate=0.051072202871981255, max_iter=32,\n",
      "                               max_leaf_nodes=377, min_samples_leaf=100,\n",
      "                               n_iter_no_change=17, random_state=1,\n",
      "                               validation_fraction=0.2801600670702926,\n",
      "                               warm_start=True)}}\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:')\n",
    "print(sklearn.metrics.accuracy_score(y_hd_ohe_test, \\\n",
    "                                     predictions_hd_ohe))\n",
    "print(' ')\n",
    "print('-----------------------------------------')\n",
    "print(' ')\n",
    "print('Sprint Stats:')\n",
    "print(automl_hd_ohe.sprint_statistics())\n",
    "print(' ')\n",
    "print('-----------------------------------------')\n",
    "print(' ')\n",
    "print('Model Details:')\n",
    "print(automl_hd_ohe.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same thing (build a model on ohe data with holdout) but this time with parallelization turned on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-08-11 23:41:38,130:Client-AutoML(5):heart_disease] Capping the per_run_time_limit to 29.0 to have time for a least 2 models in each process.\n",
      "[ERROR] [2022-08-11 23:42:35,860:asyncio.events] \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/utils.py\", line 799, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1246, in _reconnect\n",
      "    await self._ensure_connected(timeout=timeout)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1276, in _ensure_connected\n",
      "    comm = await connect(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/comm/core.py\", line 315, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/tasks.py\", line 659, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "[ERROR] [2022-08-11 23:42:35,862:asyncio.events] \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/utils.py\", line 799, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1435, in _handle_report\n",
      "    await self._reconnect()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/utils.py\", line 799, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1246, in _reconnect\n",
      "    await self._ensure_connected(timeout=timeout)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1276, in _ensure_connected\n",
      "    comm = await connect(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/comm/core.py\", line 315, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/tasks.py\", line 659, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "CPU times: user 48.4 s, sys: 2.49 s, total: 50.9 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#build the auto-sklearn routine\n",
    "automl_hd_ohe_p = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=60,\n",
    "    per_run_time_limit=30,\n",
    "    disable_evaluator_output=False,\n",
    "    resampling_strategy='holdout',\n",
    "    resampling_strategy_arguments={'train_size': 0.67},\n",
    "    \n",
    "    #turn on parallelization\n",
    "    n_jobs=4,\n",
    "    seed=5,\n",
    ")\n",
    "\n",
    "#call it\n",
    "automl_hd_ohe_p.fit(X_hd_ohe_train, y_hd_ohe_train, \\\n",
    "                    dataset_name='heart_disease')\n",
    "\n",
    "#save the predicitons\n",
    "predictions_hd_ohe_p = automl_hd_ohe_p.predict(X_hd_ohe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.6973684210526315\n",
      " \n",
      "-----------------------------------------\n",
      " \n",
      "Sprint Stats:\n",
      "auto-sklearn results:\n",
      "  Dataset name: heart_disease\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.893333\n",
      "  Number of target algorithm runs: 34\n",
      "  Number of successful target algorithm runs: 34\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:')\n",
    "print(sklearn.metrics.accuracy_score(y_hd_ohe_test, \\\n",
    "                                     predictions_hd_ohe_p))\n",
    "print(' ')\n",
    "print('-----------------------------------------')\n",
    "print(' ')\n",
    "print('Sprint Stats:')\n",
    "print(automl_hd_ohe_p.sprint_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details:\n",
      "{24: {'model_id': 24, 'rank': 1, 'cost': 0.10666666666666669, 'ensemble_weight': 0.34, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e740af0>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4e7a05b0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4e7a0130>, 'sklearn_classifier': AdaBoostClassifier(algorithm='SAMME',\n",
      "                   base_estimator=DecisionTreeClassifier(max_depth=2),\n",
      "                   learning_rate=0.13167493237005792, n_estimators=56,\n",
      "                   random_state=5)}, 13: {'model_id': 13, 'rank': 2, 'cost': 0.1333333333333333, 'ensemble_weight': 0.06, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e7b5e80>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e50fef1f0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4e2202e0>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=0.4635442279519353,\n",
      "                               learning_rate=0.09809681787962342, max_iter=512,\n",
      "                               max_leaf_nodes=328, min_samples_leaf=2,\n",
      "                               n_iter_no_change=2, random_state=5,\n",
      "                               validation_fraction=None, warm_start=True)}, 32: {'model_id': 32, 'rank': 3, 'cost': 0.1333333333333333, 'ensemble_weight': 0.16, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e51041ca0>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e45e9ea30>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e45e9e2e0>, 'sklearn_classifier': AdaBoostClassifier(algorithm='SAMME',\n",
      "                   base_estimator=DecisionTreeClassifier(max_depth=3),\n",
      "                   learning_rate=0.0901404288021253, n_estimators=291,\n",
      "                   random_state=5)}, 27: {'model_id': 27, 'rank': 4, 'cost': 0.16000000000000003, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e50fe4b50>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4e33ae80>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4e5bbb50>, 'sklearn_classifier': SGDClassifier(alpha=0.06840329554889894, eta0=7.568166542222647e-06,\n",
      "              learning_rate='constant', loss='squared_hinge', max_iter=256,\n",
      "              penalty='l1', random_state=5, tol=0.0005064155967541273,\n",
      "              warm_start=True)}, 16: {'model_id': 16, 'rank': 5, 'cost': 0.17333333333333334, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4eb90c10>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4e5bb400>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4e27e9d0>, 'sklearn_classifier': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
      "                   learning_rate=0.046269426995092074, n_estimators=406,\n",
      "                   random_state=5)}, 21: {'model_id': 21, 'rank': 6, 'cost': 0.19999999999999996, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e2af610>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e510525e0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e51052790>, 'sklearn_classifier': DecisionTreeClassifier(class_weight='balanced', max_depth=22,\n",
      "                       min_samples_leaf=4, min_samples_split=20,\n",
      "                       random_state=5)}, 35: {'model_id': 35, 'rank': 7, 'cost': 0.41333333333333333, 'ensemble_weight': 0.18, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e517910>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4e41f970>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e50ce7760>, 'sklearn_classifier': LinearSVC(C=0.14155068338652468, dual=False, intercept_scaling=1.0,\n",
      "          random_state=5, tol=0.0007817078184604042)}, 20: {'model_id': 20, 'rank': 8, 'cost': 0.43999999999999995, 'ensemble_weight': 0.12, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e98a940>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4e311670>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e45e51d30>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=0.0015246021799433245,\n",
      "                               learning_rate=0.051072202871981255, max_iter=32,\n",
      "                               max_leaf_nodes=377, min_samples_leaf=100,\n",
      "                               n_iter_no_change=17, random_state=5,\n",
      "                               validation_fraction=0.2801600670702926,\n",
      "                               warm_start=True)}}\n"
     ]
    }
   ],
   "source": [
    "print('Model Details:')\n",
    "print(automl_hd_ohe_p.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "print(sklearn.datasets.load_breast_cancer()['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from sklearn\n",
    "X_bc, y_bc = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "#build the train and test sets\n",
    "X_bc_train, X_bc_test, y_bc_train, y_bc_test = \\\n",
    "    sklearn.model_selection.train_test_split(X_bc, y_bc, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model using holdout and parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-08-11 23:42:41,972:Client-AutoML(5):breast_cancer] Capping the per_run_time_limit to 29.0 to have time for a least 2 models in each process.\n",
      "[ERROR] [2022-08-11 23:43:40,528:asyncio.events] \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/utils.py\", line 799, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1246, in _reconnect\n",
      "    await self._ensure_connected(timeout=timeout)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1276, in _ensure_connected\n",
      "    comm = await connect(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/comm/core.py\", line 315, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/tasks.py\", line 659, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "[ERROR] [2022-08-11 23:43:40,529:asyncio.events] \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/utils.py\", line 799, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1435, in _handle_report\n",
      "    await self._reconnect()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/utils.py\", line 799, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1246, in _reconnect\n",
      "    await self._ensure_connected(timeout=timeout)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/client.py\", line 1276, in _ensure_connected\n",
      "    comm = await connect(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/distributed/comm/core.py\", line 315, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/tasks.py\", line 659, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "CPU times: user 47.2 s, sys: 3.32 s, total: 50.5 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#build the auto-sklearn routine\n",
    "automl_bc = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=60,\n",
    "    per_run_time_limit=30,\n",
    "    disable_evaluator_output=False,\n",
    "    # 'holdout' with 'train_size'=0.67 is the default argument setting\n",
    "    # for AutoSklearnClassifier. It is explicitly specified in this example\n",
    "    # for demonstrational purpose.\n",
    "    resampling_strategy='holdout',\n",
    "    resampling_strategy_arguments={'train_size': 0.67},\n",
    "    n_jobs=4,\n",
    "    seed=5,\n",
    ")\n",
    "\n",
    "#call it\n",
    "automl_bc.fit(X_bc_train, y_bc_train, dataset_name='breast_cancer')\n",
    "\n",
    "#save the predicitons\n",
    "predictions_bc = automl_bc.predict(X_bc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.951048951048951\n",
      " \n",
      "-----------------------------------------\n",
      " \n",
      "Sprint Stats:\n",
      "auto-sklearn results:\n",
      "  Dataset name: breast_cancer\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.985816\n",
      "  Number of target algorithm runs: 40\n",
      "  Number of successful target algorithm runs: 40\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:')\n",
    "print(sklearn.metrics.accuracy_score(y_bc_test, \\\n",
    "                                     predictions_bc))\n",
    "print(' ')\n",
    "print('-----------------------------------------')\n",
    "print(' ')\n",
    "print('Sprint Stats:')\n",
    "print(automl_bc.sprint_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details:\n",
      "{3: {'model_id': 3, 'rank': 1, 'cost': 0.014184397163120588, 'ensemble_weight': 0.1, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e965880>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e50f94cd0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e50f94310>, 'sklearn_classifier': MLPClassifier(activation='tanh', alpha=0.0001363185819149026, beta_1=0.999,\n",
      "              beta_2=0.9, early_stopping=True,\n",
      "              hidden_layer_sizes=(115, 115, 115),\n",
      "              learning_rate_init=0.00018009776276177523, max_iter=32,\n",
      "              n_iter_no_change=32, random_state=5, verbose=0, warm_start=True)}, 7: {'model_id': 7, 'rank': 2, 'cost': 0.014184397163120588, 'ensemble_weight': 0.12, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e4a8c70>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e50fa3130>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e50fa3d60>, 'sklearn_classifier': ExtraTreesClassifier(max_features=34, min_samples_leaf=3, min_samples_split=11,\n",
      "                     n_estimators=512, n_jobs=1, random_state=5,\n",
      "                     warm_start=True)}, 2: {'model_id': 2, 'rank': 3, 'cost': 0.021276595744680882, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e45eaad90>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e142e45b0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e142e4760>, 'sklearn_classifier': RandomForestClassifier(max_features=5, n_estimators=512, n_jobs=1,\n",
      "                       random_state=5, warm_start=True)}, 16: {'model_id': 16, 'rank': 4, 'cost': 0.021276595744680882, 'ensemble_weight': 0.06, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e50ffbbb0>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e146eb3d0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e4e311af0>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=3.387912939529945e-10,\n",
      "                               learning_rate=0.30755227194768237, max_iter=128,\n",
      "                               max_leaf_nodes=60, min_samples_leaf=39,\n",
      "                               n_iter_no_change=18, random_state=5,\n",
      "                               validation_fraction=None, warm_start=True)}, 21: {'model_id': 21, 'rank': 5, 'cost': 0.021276595744680882, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e14706b50>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e4e4e6a00>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e511ced90>, 'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=4, min_samples_leaf=2,\n",
      "                     min_samples_split=15, n_estimators=512, n_jobs=1,\n",
      "                     random_state=5, warm_start=True)}, 10: {'model_id': 10, 'rank': 6, 'cost': 0.028368794326241176, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e328700>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e1477eeb0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e1477ecd0>, 'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=4, min_samples_split=6,\n",
      "                       n_estimators=512, n_jobs=1, random_state=5,\n",
      "                       warm_start=True)}, 13: {'model_id': 13, 'rank': 7, 'cost': 0.028368794326241176, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e1443ebe0>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7e142f5550>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7e142f5e80>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=False,\n",
      "                               l2_regularization=1.0647401999412075e-10,\n",
      "                               learning_rate=0.08291320147381159, max_iter=512,\n",
      "                               max_leaf_nodes=39, n_iter_no_change=0,\n",
      "                               random_state=5, validation_fraction=None,\n",
      "                               warm_start=True)}, 14: {'model_id': 14, 'rank': 8, 'cost': 0.028368794326241176, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e147323a0>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7dee0aa130>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7dedcaf700>, 'sklearn_classifier': MLPClassifier(activation='tanh', alpha=2.5550223982458062e-06, beta_1=0.999,\n",
      "              beta_2=0.9, hidden_layer_sizes=(54, 54, 54),\n",
      "              learning_rate_init=0.00027271287919467994, max_iter=256,\n",
      "              n_iter_no_change=32, random_state=5, validation_fraction=0.0,\n",
      "              verbose=0, warm_start=True)}, 22: {'model_id': 22, 'rank': 9, 'cost': 0.028368794326241176, 'ensemble_weight': 0.1, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7e4e7e5970>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7dedcaf5e0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7dedc37550>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=8.057778875694463e-05,\n",
      "                               learning_rate=0.09179220974965213, max_iter=256,\n",
      "                               max_leaf_nodes=200, n_iter_no_change=18,\n",
      "                               random_state=5,\n",
      "                               validation_fraction=0.14295295806077554,\n",
      "                               warm_start=True)}, 8: {'model_id': 8, 'rank': 10, 'cost': 0.03546099290780147, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7dee093970>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7dedafff40>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7deda8b730>, 'sklearn_classifier': RandomForestClassifier(max_features=2, min_samples_leaf=2, n_estimators=512,\n",
      "                       n_jobs=1, random_state=5, warm_start=True)}, 12: {'model_id': 12, 'rank': 11, 'cost': 0.03546099290780147, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7dedc8da60>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7deda8b8e0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7deda8bbb0>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=0.005326508887463406,\n",
      "                               learning_rate=0.060800813211425456, max_iter=512,\n",
      "                               max_leaf_nodes=6, min_samples_leaf=5,\n",
      "                               n_iter_no_change=5, random_state=5,\n",
      "                               validation_fraction=None, warm_start=True)}, 17: {'model_id': 17, 'rank': 12, 'cost': 0.03546099290780147, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7dedc17790>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7ded53e340>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7ded558910>, 'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n",
      "                               l2_regularization=0.4635442279519353,\n",
      "                               learning_rate=0.09809681787962342, max_iter=512,\n",
      "                               max_leaf_nodes=328, min_samples_leaf=2,\n",
      "                               n_iter_no_change=2, random_state=5,\n",
      "                               validation_fraction=None, warm_start=True)}, 5: {'model_id': 5, 'rank': 13, 'cost': 0.04255319148936165, 'ensemble_weight': 0.06, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded622940>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7ded558850>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7ded359be0>, 'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=3, min_samples_leaf=2,\n",
      "                       n_estimators=512, n_jobs=1, random_state=5,\n",
      "                       warm_start=True)}, 9: {'model_id': 9, 'rank': 14, 'cost': 0.04255319148936165, 'ensemble_weight': 0.04, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded5c4d30>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7ded359f10>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7ded256f10>, 'sklearn_classifier': ExtraTreesClassifier(max_features=7, min_samples_split=10, n_estimators=512,\n",
      "                     n_jobs=1, random_state=5, warm_start=True)}, 24: {'model_id': 24, 'rank': 15, 'cost': 0.07092198581560283, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded544d00>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7ded182070>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7ded182d90>, 'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=16, n_estimators=512,\n",
      "                       n_jobs=1, random_state=5, warm_start=True)}, 41: {'model_id': 41, 'rank': 16, 'cost': 0.07092198581560283, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded3b1940>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7ded0c95e0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7ded0c99a0>, 'sklearn_classifier': PassiveAggressiveClassifier(C=0.0009253539563156188, max_iter=32,\n",
      "                            random_state=5, tol=0.0010615221402888297,\n",
      "                            warm_start=True)}, 37: {'model_id': 37, 'rank': 17, 'cost': 0.12056737588652477, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded284100>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7decf51070>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7decf51340>, 'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, max_features=1, min_samples_leaf=4,\n",
      "                     min_samples_split=5, n_estimators=512, n_jobs=1,\n",
      "                     random_state=5, warm_start=True)}, 31: {'model_id': 31, 'rank': 18, 'cost': 0.14893617021276595, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded141250>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7dece4bc70>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7dece765b0>, 'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, criterion='entropy', max_features=1,\n",
      "                     min_samples_leaf=14, min_samples_split=4, n_estimators=512,\n",
      "                     n_jobs=1, random_state=5, warm_start=True)}, 35: {'model_id': 35, 'rank': 19, 'cost': 0.3120567375886525, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7ded0a2e20>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7decc4a4c0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7decc4ad60>, 'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=1, min_samples_leaf=11,\n",
      "                     min_samples_split=7, n_estimators=512, n_jobs=1,\n",
      "                     random_state=5, warm_start=True)}, 38: {'model_id': 38, 'rank': 20, 'cost': 0.36879432624113473, 'ensemble_weight': 0.08, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7decf1ab20>, 'balancing': Balancing(random_state=5), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7deca0d2e0>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7deca0d610>, 'sklearn_classifier': SGDClassifier(alpha=0.000271809619218857, eta0=8.238386231147772e-06,\n",
      "              l1_ratio=0.0019340435025409653, learning_rate='invscaling',\n",
      "              max_iter=16, penalty='elasticnet', power_t=0.3965825827722685,\n",
      "              random_state=5, tol=0.018041492299053326, warm_start=True)}, 28: {'model_id': 28, 'rank': 21, 'cost': 0.5957446808510638, 'ensemble_weight': 0.02, 'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f7decd624f0>, 'balancing': Balancing(random_state=5, strategy='weighting'), 'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f7dec88ce50>, 'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f7dec89b160>, 'sklearn_classifier': BernoulliNB(alpha=0.11459185843611619, fit_prior=False)}}\n"
     ]
    }
   ],
   "source": [
    "print('Model Details:')\n",
    "print(automl_bc.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Run and Accuracy Stats\n",
    "\n",
    "All in one place for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Heart Disease---------------\n",
      " \n",
      " \n",
      "Model stats HD Holdout:\n",
      "auto-sklearn results:\n",
      "  Dataset name: heart_disease\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.880000\n",
      "  Number of target algorithm runs: 25\n",
      "  Number of successful target algorithm runs: 25\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      " \n",
      "Accuracy score HD Holdout:\n",
      "0.6973684210526315\n",
      " \n",
      "-----------------------------------------\n",
      " \n",
      "Model stats HD Holdout Parallel:\n",
      "auto-sklearn results:\n",
      "  Dataset name: heart_disease\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.893333\n",
      "  Number of target algorithm runs: 34\n",
      "  Number of successful target algorithm runs: 34\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "Accuracy score HD Holdout Parallel:\n",
      "0.6973684210526315\n",
      " \n",
      "-----------------------------------------\n",
      " \n",
      " \n",
      " \n",
      "-----------Breast Cancer---------------\n",
      " \n",
      " \n",
      "Model stats BC Holdout Parallel:\n",
      "auto-sklearn results:\n",
      "  Dataset name: breast_cancer\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.985816\n",
      "  Number of target algorithm runs: 40\n",
      "  Number of successful target algorithm runs: 40\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      " \n",
      "Accuracy score BC Holdout Parallel:\n",
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------Heart Disease---------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "print(\"Model stats HD Holdout:\")\n",
    "print(automl_hd_ohe.sprint_statistics())\n",
    "print(' ')\n",
    "print(\"Accuracy score HD Holdout:\")\n",
    "print(sklearn.metrics.accuracy_score(y_hd_ohe_test, \\\n",
    "                                     predictions_hd_ohe))\n",
    "\n",
    "print(' ')\n",
    "print('-----------------------------------------')\n",
    "print(' ')\n",
    "\n",
    "print(\"Model stats HD Holdout Parallel:\")\n",
    "print(automl_hd_ohe_p.sprint_statistics())\n",
    "print(\"Accuracy score HD Holdout Parallel:\")\n",
    "print(sklearn.metrics.accuracy_score(y_hd_ohe_test, \\\n",
    "                                     predictions_hd_ohe_p))\n",
    "\n",
    "print(' ')\n",
    "print('-----------------------------------------')\n",
    "print(' ')\n",
    "\n",
    "# #holdout parallel feat_type\n",
    "# print(\"Model stats HD Holdout Feature Type Parallel:\")\n",
    "# print(automl_hd_ft_p.sprint_statistics())\n",
    "# print(' ')\n",
    "# print(\"Accuracy score HD Holdout Feature Type Parallel:\")\n",
    "# print(sklearn.metrics.accuracy_score(y_hd_test, \\\n",
    "#                                      predictions_hd_ft_p))\n",
    "\n",
    "# print(' ')\n",
    "# print('-----------------------------------------')\n",
    "# print(' ')\n",
    "\n",
    "# #cross validation parallel\n",
    "# print(\"Model stats HD CV Parllel:\")\n",
    "# print(automl_hd_cv_p.sprint_statistics())\n",
    "# print(' ')\n",
    "# print(\"Accuracy score HD CV Parallel:\")\n",
    "# print(sklearn.metrics.accuracy_score(y_hd_ohe_test, \\\n",
    "#                                      predictions_hd_cv_p))\n",
    "\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "print(\"-----------Breast Cancer---------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "print(\"Model stats BC Holdout Parallel:\")\n",
    "print(automl_bc.sprint_statistics())\n",
    "print(' ')\n",
    "print(\"Accuracy score BC Holdout Parallel:\")\n",
    "print(sklearn.metrics.accuracy_score(y_bc_test, \\\n",
    "                                     predictions_bc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Ensemble to Disk\n",
    "\n",
    "In the specific case of [scikit-learn](https://scikit-learn.org/stable/modules/model_persistence.html#persistence-example), it may be better to use joblib’s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autosklearn_bc.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(automl_bc, 'autosklearn_bc.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Domino Stats File\n",
    "\n",
    "To keep things simple, we pick one of the hd models. Saving stats to this file [allows Domino to track and trend them in the Experiment Manager](https://support.dominodatalab.com/hc/en-us/articles/204348169-Diagnostic-statistics-with-dominostats-json) when this notebook is run as a batch or scheduled job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_acc = sklearn.metrics.accuracy_score(y_hd_ohe_test, \\\n",
    "                                        predictions_hd_ohe_p)\n",
    "bc_acc = sklearn.metrics.accuracy_score(y_bc_test, \\\n",
    "                                        predictions_bc)\n",
    "\n",
    "import json\n",
    "with open('dominostats.json', 'w') as f:\n",
    "    f.write(json.dumps( {\"HD_ACC\": hd_acc, \"BC_ACC\": bc_acc}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
